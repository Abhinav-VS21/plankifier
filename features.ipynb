{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column is  genus\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import keras, os, time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "filename='./data/feature-data-dummy/0P5X_euk.txt'\n",
    "target='genus'\n",
    "print('Target column is ',target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (4172, 218)\n",
      "Number of cols 218\n",
      "Number of rows 4172\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "X=pd.read_csv(filename,sep='\\t',low_memory=False)\n",
    "print('Shape of the dataset:',X.shape)\n",
    "print('Number of cols',len(X.columns))\n",
    "print('Number of rows',len(X.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing rows with no target: (3586, 218)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows without target\n",
    "X.dropna(axis=0,subset=[target], inplace=True)\n",
    "print('Shape after removing rows with no target:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data from labels\n",
    "y = X[target]\n",
    "X.drop([target], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping empty columns (3586, 103)\n"
     ]
    }
   ],
   "source": [
    "# Remove empty columns\n",
    "empty_cols=[col for col in X.columns if X[col].count()==0]\n",
    "if len(empty_cols)>0:\n",
    "    X.drop(empty_cols,axis=1,inplace=True)\n",
    "print(\"Shape after dropping empty columns\",X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing the bad columns: (3586, 67)\n"
     ]
    }
   ],
   "source": [
    "# Remove other columns that shouldn't be used\n",
    "\n",
    "# Columns that are good for the dataset\n",
    "size_cols=['maj_axis_len','min_axis_len','area','aspect_ratio','eccentricity','estimated_volume','file_size','image_height','image_width','orientation','solidity',]\n",
    "color_cols=sorted(X.loc[:, X.columns.str.contains('intensity')])\n",
    "\n",
    "# Columns that should absolutely be removed\n",
    "leakage_cols=['class','species','family','empire','kingdom','order','phylum']\n",
    "\n",
    "# Other columns\n",
    "time_cols=['acquisition_time']\n",
    "useless_cols=sorted(X.loc[:, X.columns.str.contains('modif')])+['_id','extension','filename','group_id','tags','upload_id']\n",
    "boh_cols=['multiple_species','partially_cropped'] # These columns are mostly nan, but I suspect that these nan should be False\n",
    "\n",
    "\n",
    "keep_cols=size_cols+color_cols\n",
    "remove_cols=leakage_cols+useless_cols+time_cols+boh_cols\n",
    "\n",
    "\n",
    "remaining_cols=[col for col in X.columns if col not in remove_cols+keep_cols]\n",
    "if len(remaining_cols)!=0:\n",
    "    print('There are still some columns that you didn\\'t take into account!')\n",
    "    print(remaining_cols)\n",
    "\n",
    "X.drop(remove_cols,axis=1,inplace=True)\n",
    "\n",
    "print('Shape after removing the bad columns:',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with categorical values\n",
    "categoric_cols = [col for col in X.columns if X[col].dtype=='object']\n",
    "int_cols = [col for col in X.columns if X[col].dtype=='int']\n",
    "float_cols = [col for col in X.columns if X[col].dtype=='float']\n",
    "\n",
    "if len(int_cols+float_cols+categoric_cols)!=len(X.columns):\n",
    "    print('There are still some types that you didn\\'t take into account!')\n",
    "    print(set([X[col].dtype for col in X.columns]))\n",
    "\n",
    "X.drop(categoric_cols,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test\n",
    "X=X.values # If I work with numpy arrays I can use verbatim the code of the other models\n",
    "y=y.values\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_size=len(trainX)\n",
    "test_size=len(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform labels into vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# Define MLP\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, input_shape=(len(trainX[0]),), activation=\"sigmoid\"))\n",
    "model.add(Dense(512, activation=\"sigmoid\"))\n",
    "model.add(Dense(len(lb.classes_), activation=\"softmax\"))\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.1,nesterov=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "start=time.time()\n",
    "history = model.fit(\n",
    "    trainX, trainY, batch_size=32, \n",
    "    validation_data=(testX, testY), \n",
    "    epochs=20)\n",
    "trainingTime=time.time()-start\n",
    "print('Training took',trainingTime/60,'minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate the network\n",
    "predictions = model.predict(testX, batch_size=32)\n",
    "clrep=classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=lb.classes_)\n",
    "print(clrep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
