{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for predictions \n",
    "\n",
    "Here, we assume that there is a saved model which we want to use for predictions, and use it on a target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, keras, re, pathlib, numpy as np\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from src import helper_models, helper_data\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will predict the class of 535 images belonging to the daphnia class\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "modelpath='./out/conv2/2020-01-19_22h44m18s/'\n",
    "modelpath='./out/conv2/2020-01-21_17h11m33s/'\n",
    "\n",
    "argsname=modelpath+'args.txt'\n",
    "\n",
    "# Test on Daphnia\n",
    "target='daphnia'\n",
    "im_names=['./data/zooplankton_trainingset_15oct/'+target+'/'+name for name in os.listdir('./data/zooplankton_trainingset_15oct/'+target+'/') if '.jpeg' in name]\n",
    "print('We will predict the class of {} images belonging to the {} class'.format(len(im_names),target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(aug=True, bs=8, datapath='./data/zooplankton_trainingset_15oct/', depth=3, height=128, layers=[256, 128], load=None, lr=0.0001, model='conv2', opt='sgd', outpath='./out/', plot=False, resize='keep_proportions', testSplit=0.2, totEpochs=300, verbose=False, width=128)\n",
      "Training-time: 10629.327492952347 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read Arguments\n",
    "with open(argsname,'r') as fargs:\n",
    "    args=fargs.read()\n",
    "print(args)\n",
    "layers=[None,None]\n",
    "for s in re.split('[\\,,\\),\\(]',args):\n",
    "    if 'height' in s:\n",
    "        height=np.int64(re.search('(\\d+)',s).group(1))\n",
    "    if 'width' in s:\n",
    "        width=np.int64(re.search('(\\d+)',s).group(1))\n",
    "    if 'depth' in s:\n",
    "        depth=np.int64(re.search('(\\d+)',s).group(1))\n",
    "    if 'model' in s:\n",
    "        modelname=re.search('=\\'(.+)\\'$',s).group(1)\n",
    "    if 'resize' in s:\n",
    "        resize=re.search('=\\'(.+)\\'$',s).group(1)\n",
    "    if 'layers' in s: #first layer\n",
    "        layers[0]=np.int64(re.search('=\\[(.+)$',s).group(1))\n",
    "    if re.match('^ \\d+',s): #second layer\n",
    "        layers[1]=np.int64(re.match('^ (\\d+)',s).group(1))\n",
    "    if 'datapath' in s:\n",
    "        datapath=re.search('=\\'(.+)\\'$',s).group(1)\n",
    "    if 'outpath' in s:\n",
    "        outpath=re.search('=\\'(.+)\\'$',s).group(1)\n",
    "    \n",
    "classes = [ name for name in os.listdir(datapath) if os.path.isdir(os.path.join(datapath, name)) ]\n",
    "\n",
    "# Double check that classes are correct\n",
    "classes_dict=np.load(modelpath+'classes.npy',allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: conv2\n",
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/miniconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Loaded  ./out/conv2/2020-01-21_17h11m33s//bestweights.hdf5\n"
     ]
    }
   ],
   "source": [
    "from stat import S_ISREG, ST_CTIME, ST_MODE\n",
    "# Choose model. We load the latest created .hdf5 file, since later is better\n",
    "entries = [modelpath+'/'+entry for entry in os.listdir(modelpath) if '.hdf5' in entry]\n",
    "entries = ((os.stat(path), path) for path in entries)\n",
    "entries = ((stat[ST_CTIME], path) for stat, path in entries if S_ISREG(stat[ST_MODE]))\n",
    "modelfile=sorted(entries)[-1][1]\n",
    "\n",
    "# Initialize and load model\n",
    "print('Model:',modelname)\n",
    "model=keras.models.load_model(modelfile)\n",
    "\n",
    "# model = keras.models.load_model(modelfile)\n",
    "# opt = keras.optimizers.SGD(lr=0.001, nesterov=True)\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "print('Loaded ',modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(im_names, width, height, depth, modelname, resize):\n",
    "    ''' \n",
    "    Function that loads a list of images given in im_names, and returns \n",
    "    them in a numpy format that can be used by the classifier\n",
    "    '''\n",
    "    npimages=np.ndarray((len(im_names),width,height,depth)) if modelname != 'mlp' else np.ndarray((len(im_names),width*height*depth))\n",
    "\n",
    "    for i,im_name in enumerate(im_names):\n",
    "        image = Image.open(im_name)\n",
    "        if resize == 'acazzo':\n",
    "            image = image.resize((width,height))\n",
    "        else:\n",
    "            # Set image's largest dimension to target size, and fill the rest with black pixels\n",
    "            image,rescaled = helper_data.ResizeWithProportions(image, width) # width and height are assumed to be the same (assertion at the beginning)\n",
    "            npimage=np.array(image.copy())\n",
    "        if model == 'mlp':\n",
    "            npimage = npimage.flatten()\n",
    "        npimages[i]=npimage\n",
    "        if len(im_names)==1:\n",
    "            image.show()\n",
    "        image.close()    \n",
    "    return npimages/255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "npimages=load_images(im_names, width, height, depth, modelname, resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8411214953271028\n"
     ]
    }
   ],
   "source": [
    "# Print prediction\n",
    "predictions=model.predict(npimages).argmax(axis=1)\n",
    "count=0\n",
    "for i in range(len(npimages)):\n",
    "    if classes_dict['name'][predictions[i]] == 'daphnia':\n",
    "        count+=1\n",
    "print('Accuracy: ', count/len(npimages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
